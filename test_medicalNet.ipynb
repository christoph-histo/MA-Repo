{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import video\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.append('/home/christoph/Dokumente/christoph-MA/MA-Repo')\n",
    "sys.path.append('/home/christoph/Dokumente/christoph-MA/MedicalNet/models')\n",
    "sys.path.append('/home/christoph/Dokumente/christoph-MA/research-contributions/SwinUNETR')\n",
    "import resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.resnet18()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christoph/Dokumente/christoph-MA/MedicalNet/models/resnet.py:173: FutureWarning: `nn.init.kaiming_normal` is now deprecated in favor of `nn.init.kaiming_normal_`.\n",
      "  m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = resnet.resnet18(sample_input_D=32,sample_input_H=128,sample_input_W=128,num_seg_classes=3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import DataParallel, Sequential, AdaptiveAvgPool3d, Linear\n",
    "\n",
    "model = resnet.resnet18(sample_input_D=32, sample_input_H=128, sample_input_W=128, num_seg_classes=3) \n",
    "\n",
    "model = DataParallel(model) \n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "model.module.conv_seg = Sequential(\n",
    "    AdaptiveAvgPool3d(output_size=(1, 1, 1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): ResNet(\n",
      "    (conv1): Conv3d(1, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
      "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(2, 2, 2), dilation=(2, 2, 2), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(2, 2, 2), dilation=(2, 2, 2), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(2, 2, 2), dilation=(2, 2, 2), bias=False)\n",
      "        (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(2, 2, 2), dilation=(2, 2, 2), bias=False)\n",
      "        (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n",
      "        (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n",
      "        (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (conv_seg): Sequential(\n",
      "      (0): ConvTranspose3d(512, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv3d(32, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a random tensor dim (1, 3, 32, 128, 128)\n",
    "input_tensor = torch.randn(1, 1, 32, 128, 128).to('cuda')\n",
    "\n",
    "#forward pass\n",
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 512, 1, 1, 1])\n",
      "Output: tensor([[[[[0.8251]]],\n",
      "\n",
      "\n",
      "         [[[0.7998]]],\n",
      "\n",
      "\n",
      "         [[[0.8235]]],\n",
      "\n",
      "\n",
      "         [[[0.8770]]],\n",
      "\n",
      "\n",
      "         [[[0.8651]]],\n",
      "\n",
      "\n",
      "         [[[0.8210]]],\n",
      "\n",
      "\n",
      "         [[[0.8684]]],\n",
      "\n",
      "\n",
      "         [[[0.8422]]],\n",
      "\n",
      "\n",
      "         [[[0.8146]]],\n",
      "\n",
      "\n",
      "         [[[0.8232]]],\n",
      "\n",
      "\n",
      "         [[[0.8459]]],\n",
      "\n",
      "\n",
      "         [[[0.8293]]],\n",
      "\n",
      "\n",
      "         [[[0.8078]]],\n",
      "\n",
      "\n",
      "         [[[0.8033]]],\n",
      "\n",
      "\n",
      "         [[[0.8020]]],\n",
      "\n",
      "\n",
      "         [[[0.8342]]],\n",
      "\n",
      "\n",
      "         [[[0.8269]]],\n",
      "\n",
      "\n",
      "         [[[0.8191]]],\n",
      "\n",
      "\n",
      "         [[[0.8290]]],\n",
      "\n",
      "\n",
      "         [[[0.8209]]],\n",
      "\n",
      "\n",
      "         [[[0.8056]]],\n",
      "\n",
      "\n",
      "         [[[0.8470]]],\n",
      "\n",
      "\n",
      "         [[[0.8140]]],\n",
      "\n",
      "\n",
      "         [[[0.8084]]],\n",
      "\n",
      "\n",
      "         [[[0.8016]]],\n",
      "\n",
      "\n",
      "         [[[0.8223]]],\n",
      "\n",
      "\n",
      "         [[[0.7877]]],\n",
      "\n",
      "\n",
      "         [[[0.8215]]],\n",
      "\n",
      "\n",
      "         [[[0.8755]]],\n",
      "\n",
      "\n",
      "         [[[0.8091]]],\n",
      "\n",
      "\n",
      "         [[[0.8423]]],\n",
      "\n",
      "\n",
      "         [[[0.8477]]],\n",
      "\n",
      "\n",
      "         [[[0.8026]]],\n",
      "\n",
      "\n",
      "         [[[0.8021]]],\n",
      "\n",
      "\n",
      "         [[[0.7811]]],\n",
      "\n",
      "\n",
      "         [[[0.7996]]],\n",
      "\n",
      "\n",
      "         [[[0.8550]]],\n",
      "\n",
      "\n",
      "         [[[0.8020]]],\n",
      "\n",
      "\n",
      "         [[[0.8783]]],\n",
      "\n",
      "\n",
      "         [[[0.8429]]],\n",
      "\n",
      "\n",
      "         [[[0.8484]]],\n",
      "\n",
      "\n",
      "         [[[0.8151]]],\n",
      "\n",
      "\n",
      "         [[[0.8160]]],\n",
      "\n",
      "\n",
      "         [[[0.8230]]],\n",
      "\n",
      "\n",
      "         [[[0.8297]]],\n",
      "\n",
      "\n",
      "         [[[0.8724]]],\n",
      "\n",
      "\n",
      "         [[[0.7682]]],\n",
      "\n",
      "\n",
      "         [[[0.8551]]],\n",
      "\n",
      "\n",
      "         [[[0.7617]]],\n",
      "\n",
      "\n",
      "         [[[0.8689]]],\n",
      "\n",
      "\n",
      "         [[[0.8137]]],\n",
      "\n",
      "\n",
      "         [[[0.7935]]],\n",
      "\n",
      "\n",
      "         [[[0.8381]]],\n",
      "\n",
      "\n",
      "         [[[0.8002]]],\n",
      "\n",
      "\n",
      "         [[[0.8362]]],\n",
      "\n",
      "\n",
      "         [[[0.8576]]],\n",
      "\n",
      "\n",
      "         [[[0.8398]]],\n",
      "\n",
      "\n",
      "         [[[0.8060]]],\n",
      "\n",
      "\n",
      "         [[[0.8152]]],\n",
      "\n",
      "\n",
      "         [[[0.8160]]],\n",
      "\n",
      "\n",
      "         [[[0.8068]]],\n",
      "\n",
      "\n",
      "         [[[0.8186]]],\n",
      "\n",
      "\n",
      "         [[[0.8173]]],\n",
      "\n",
      "\n",
      "         [[[0.8459]]],\n",
      "\n",
      "\n",
      "         [[[0.7756]]],\n",
      "\n",
      "\n",
      "         [[[0.8130]]],\n",
      "\n",
      "\n",
      "         [[[0.8392]]],\n",
      "\n",
      "\n",
      "         [[[0.8268]]],\n",
      "\n",
      "\n",
      "         [[[0.8772]]],\n",
      "\n",
      "\n",
      "         [[[0.8212]]],\n",
      "\n",
      "\n",
      "         [[[0.8028]]],\n",
      "\n",
      "\n",
      "         [[[0.8101]]],\n",
      "\n",
      "\n",
      "         [[[0.8163]]],\n",
      "\n",
      "\n",
      "         [[[0.8323]]],\n",
      "\n",
      "\n",
      "         [[[0.8038]]],\n",
      "\n",
      "\n",
      "         [[[0.8220]]],\n",
      "\n",
      "\n",
      "         [[[0.8259]]],\n",
      "\n",
      "\n",
      "         [[[0.7987]]],\n",
      "\n",
      "\n",
      "         [[[0.8095]]],\n",
      "\n",
      "\n",
      "         [[[0.8071]]],\n",
      "\n",
      "\n",
      "         [[[0.8281]]],\n",
      "\n",
      "\n",
      "         [[[0.8244]]],\n",
      "\n",
      "\n",
      "         [[[0.7819]]],\n",
      "\n",
      "\n",
      "         [[[0.8024]]],\n",
      "\n",
      "\n",
      "         [[[0.8040]]],\n",
      "\n",
      "\n",
      "         [[[0.8245]]],\n",
      "\n",
      "\n",
      "         [[[0.8411]]],\n",
      "\n",
      "\n",
      "         [[[0.7906]]],\n",
      "\n",
      "\n",
      "         [[[0.8509]]],\n",
      "\n",
      "\n",
      "         [[[0.8227]]],\n",
      "\n",
      "\n",
      "         [[[0.8188]]],\n",
      "\n",
      "\n",
      "         [[[0.8477]]],\n",
      "\n",
      "\n",
      "         [[[0.8343]]],\n",
      "\n",
      "\n",
      "         [[[0.7489]]],\n",
      "\n",
      "\n",
      "         [[[0.8159]]],\n",
      "\n",
      "\n",
      "         [[[0.8459]]],\n",
      "\n",
      "\n",
      "         [[[0.8302]]],\n",
      "\n",
      "\n",
      "         [[[0.8262]]],\n",
      "\n",
      "\n",
      "         [[[0.8055]]],\n",
      "\n",
      "\n",
      "         [[[0.7825]]],\n",
      "\n",
      "\n",
      "         [[[0.8298]]],\n",
      "\n",
      "\n",
      "         [[[0.8879]]],\n",
      "\n",
      "\n",
      "         [[[0.8033]]],\n",
      "\n",
      "\n",
      "         [[[0.8185]]],\n",
      "\n",
      "\n",
      "         [[[0.8268]]],\n",
      "\n",
      "\n",
      "         [[[0.8152]]],\n",
      "\n",
      "\n",
      "         [[[0.8318]]],\n",
      "\n",
      "\n",
      "         [[[0.8589]]],\n",
      "\n",
      "\n",
      "         [[[0.7951]]],\n",
      "\n",
      "\n",
      "         [[[0.8721]]],\n",
      "\n",
      "\n",
      "         [[[0.8516]]],\n",
      "\n",
      "\n",
      "         [[[0.8341]]],\n",
      "\n",
      "\n",
      "         [[[0.8573]]],\n",
      "\n",
      "\n",
      "         [[[0.8015]]],\n",
      "\n",
      "\n",
      "         [[[0.8317]]],\n",
      "\n",
      "\n",
      "         [[[0.7954]]],\n",
      "\n",
      "\n",
      "         [[[0.8420]]],\n",
      "\n",
      "\n",
      "         [[[0.8031]]],\n",
      "\n",
      "\n",
      "         [[[0.8494]]],\n",
      "\n",
      "\n",
      "         [[[0.8418]]],\n",
      "\n",
      "\n",
      "         [[[0.7923]]],\n",
      "\n",
      "\n",
      "         [[[0.8423]]],\n",
      "\n",
      "\n",
      "         [[[0.8316]]],\n",
      "\n",
      "\n",
      "         [[[0.8120]]],\n",
      "\n",
      "\n",
      "         [[[0.8333]]],\n",
      "\n",
      "\n",
      "         [[[0.8204]]],\n",
      "\n",
      "\n",
      "         [[[0.8558]]],\n",
      "\n",
      "\n",
      "         [[[0.7642]]],\n",
      "\n",
      "\n",
      "         [[[0.8181]]],\n",
      "\n",
      "\n",
      "         [[[0.7935]]],\n",
      "\n",
      "\n",
      "         [[[0.7808]]],\n",
      "\n",
      "\n",
      "         [[[0.8224]]],\n",
      "\n",
      "\n",
      "         [[[0.8021]]],\n",
      "\n",
      "\n",
      "         [[[0.8581]]],\n",
      "\n",
      "\n",
      "         [[[0.8368]]],\n",
      "\n",
      "\n",
      "         [[[0.8130]]],\n",
      "\n",
      "\n",
      "         [[[0.7864]]],\n",
      "\n",
      "\n",
      "         [[[0.8585]]],\n",
      "\n",
      "\n",
      "         [[[0.8335]]],\n",
      "\n",
      "\n",
      "         [[[0.8084]]],\n",
      "\n",
      "\n",
      "         [[[0.8198]]],\n",
      "\n",
      "\n",
      "         [[[0.7964]]],\n",
      "\n",
      "\n",
      "         [[[0.8151]]],\n",
      "\n",
      "\n",
      "         [[[0.8467]]],\n",
      "\n",
      "\n",
      "         [[[0.8083]]],\n",
      "\n",
      "\n",
      "         [[[0.8301]]],\n",
      "\n",
      "\n",
      "         [[[0.7691]]],\n",
      "\n",
      "\n",
      "         [[[0.7785]]],\n",
      "\n",
      "\n",
      "         [[[0.7670]]],\n",
      "\n",
      "\n",
      "         [[[0.8119]]],\n",
      "\n",
      "\n",
      "         [[[0.8365]]],\n",
      "\n",
      "\n",
      "         [[[0.7773]]],\n",
      "\n",
      "\n",
      "         [[[0.8056]]],\n",
      "\n",
      "\n",
      "         [[[0.8494]]],\n",
      "\n",
      "\n",
      "         [[[0.8324]]],\n",
      "\n",
      "\n",
      "         [[[0.8677]]],\n",
      "\n",
      "\n",
      "         [[[0.7994]]],\n",
      "\n",
      "\n",
      "         [[[0.7908]]],\n",
      "\n",
      "\n",
      "         [[[0.8617]]],\n",
      "\n",
      "\n",
      "         [[[0.8401]]],\n",
      "\n",
      "\n",
      "         [[[0.8044]]],\n",
      "\n",
      "\n",
      "         [[[0.8476]]],\n",
      "\n",
      "\n",
      "         [[[0.7960]]],\n",
      "\n",
      "\n",
      "         [[[0.8030]]],\n",
      "\n",
      "\n",
      "         [[[0.7855]]],\n",
      "\n",
      "\n",
      "         [[[0.8037]]],\n",
      "\n",
      "\n",
      "         [[[0.8327]]],\n",
      "\n",
      "\n",
      "         [[[0.8371]]],\n",
      "\n",
      "\n",
      "         [[[0.8130]]],\n",
      "\n",
      "\n",
      "         [[[0.8096]]],\n",
      "\n",
      "\n",
      "         [[[0.8078]]],\n",
      "\n",
      "\n",
      "         [[[0.8323]]],\n",
      "\n",
      "\n",
      "         [[[0.8171]]],\n",
      "\n",
      "\n",
      "         [[[0.8075]]],\n",
      "\n",
      "\n",
      "         [[[0.8495]]],\n",
      "\n",
      "\n",
      "         [[[0.8230]]],\n",
      "\n",
      "\n",
      "         [[[0.8016]]],\n",
      "\n",
      "\n",
      "         [[[0.7984]]],\n",
      "\n",
      "\n",
      "         [[[0.8150]]],\n",
      "\n",
      "\n",
      "         [[[0.8197]]],\n",
      "\n",
      "\n",
      "         [[[0.8210]]],\n",
      "\n",
      "\n",
      "         [[[0.8890]]],\n",
      "\n",
      "\n",
      "         [[[0.8200]]],\n",
      "\n",
      "\n",
      "         [[[0.8014]]],\n",
      "\n",
      "\n",
      "         [[[0.8085]]],\n",
      "\n",
      "\n",
      "         [[[0.8436]]],\n",
      "\n",
      "\n",
      "         [[[0.8381]]],\n",
      "\n",
      "\n",
      "         [[[0.8417]]],\n",
      "\n",
      "\n",
      "         [[[0.7771]]],\n",
      "\n",
      "\n",
      "         [[[0.8291]]],\n",
      "\n",
      "\n",
      "         [[[0.8171]]],\n",
      "\n",
      "\n",
      "         [[[0.8302]]],\n",
      "\n",
      "\n",
      "         [[[0.8559]]],\n",
      "\n",
      "\n",
      "         [[[0.8187]]],\n",
      "\n",
      "\n",
      "         [[[0.8080]]],\n",
      "\n",
      "\n",
      "         [[[0.8132]]],\n",
      "\n",
      "\n",
      "         [[[0.8052]]],\n",
      "\n",
      "\n",
      "         [[[0.7916]]],\n",
      "\n",
      "\n",
      "         [[[0.8334]]],\n",
      "\n",
      "\n",
      "         [[[0.7845]]],\n",
      "\n",
      "\n",
      "         [[[0.7910]]],\n",
      "\n",
      "\n",
      "         [[[0.8100]]],\n",
      "\n",
      "\n",
      "         [[[0.8069]]],\n",
      "\n",
      "\n",
      "         [[[0.8329]]],\n",
      "\n",
      "\n",
      "         [[[0.8214]]],\n",
      "\n",
      "\n",
      "         [[[0.8141]]],\n",
      "\n",
      "\n",
      "         [[[0.8115]]],\n",
      "\n",
      "\n",
      "         [[[0.8248]]],\n",
      "\n",
      "\n",
      "         [[[0.8287]]],\n",
      "\n",
      "\n",
      "         [[[0.8277]]],\n",
      "\n",
      "\n",
      "         [[[0.8293]]],\n",
      "\n",
      "\n",
      "         [[[0.8696]]],\n",
      "\n",
      "\n",
      "         [[[0.7941]]],\n",
      "\n",
      "\n",
      "         [[[0.8287]]],\n",
      "\n",
      "\n",
      "         [[[0.8168]]],\n",
      "\n",
      "\n",
      "         [[[0.8082]]],\n",
      "\n",
      "\n",
      "         [[[0.8316]]],\n",
      "\n",
      "\n",
      "         [[[0.8105]]],\n",
      "\n",
      "\n",
      "         [[[0.8159]]],\n",
      "\n",
      "\n",
      "         [[[0.8802]]],\n",
      "\n",
      "\n",
      "         [[[0.8428]]],\n",
      "\n",
      "\n",
      "         [[[0.8113]]],\n",
      "\n",
      "\n",
      "         [[[0.8066]]],\n",
      "\n",
      "\n",
      "         [[[0.8193]]],\n",
      "\n",
      "\n",
      "         [[[0.8363]]],\n",
      "\n",
      "\n",
      "         [[[0.8014]]],\n",
      "\n",
      "\n",
      "         [[[0.8161]]],\n",
      "\n",
      "\n",
      "         [[[0.8253]]],\n",
      "\n",
      "\n",
      "         [[[0.8175]]],\n",
      "\n",
      "\n",
      "         [[[0.8306]]],\n",
      "\n",
      "\n",
      "         [[[0.7898]]],\n",
      "\n",
      "\n",
      "         [[[0.8285]]],\n",
      "\n",
      "\n",
      "         [[[0.8513]]],\n",
      "\n",
      "\n",
      "         [[[0.8025]]],\n",
      "\n",
      "\n",
      "         [[[0.8396]]],\n",
      "\n",
      "\n",
      "         [[[0.8294]]],\n",
      "\n",
      "\n",
      "         [[[0.7764]]],\n",
      "\n",
      "\n",
      "         [[[0.7995]]],\n",
      "\n",
      "\n",
      "         [[[0.8321]]],\n",
      "\n",
      "\n",
      "         [[[0.7769]]],\n",
      "\n",
      "\n",
      "         [[[0.8322]]],\n",
      "\n",
      "\n",
      "         [[[0.8091]]],\n",
      "\n",
      "\n",
      "         [[[0.8216]]],\n",
      "\n",
      "\n",
      "         [[[0.8260]]],\n",
      "\n",
      "\n",
      "         [[[0.8119]]],\n",
      "\n",
      "\n",
      "         [[[0.8479]]],\n",
      "\n",
      "\n",
      "         [[[0.8074]]],\n",
      "\n",
      "\n",
      "         [[[0.8022]]],\n",
      "\n",
      "\n",
      "         [[[0.8057]]],\n",
      "\n",
      "\n",
      "         [[[0.8293]]],\n",
      "\n",
      "\n",
      "         [[[0.8052]]],\n",
      "\n",
      "\n",
      "         [[[0.8277]]],\n",
      "\n",
      "\n",
      "         [[[0.8042]]],\n",
      "\n",
      "\n",
      "         [[[0.8169]]],\n",
      "\n",
      "\n",
      "         [[[0.7999]]],\n",
      "\n",
      "\n",
      "         [[[0.7689]]],\n",
      "\n",
      "\n",
      "         [[[0.8320]]],\n",
      "\n",
      "\n",
      "         [[[0.8273]]],\n",
      "\n",
      "\n",
      "         [[[0.8339]]],\n",
      "\n",
      "\n",
      "         [[[0.8217]]],\n",
      "\n",
      "\n",
      "         [[[0.8461]]],\n",
      "\n",
      "\n",
      "         [[[0.8272]]],\n",
      "\n",
      "\n",
      "         [[[0.7966]]],\n",
      "\n",
      "\n",
      "         [[[0.8007]]],\n",
      "\n",
      "\n",
      "         [[[0.8163]]],\n",
      "\n",
      "\n",
      "         [[[0.7831]]],\n",
      "\n",
      "\n",
      "         [[[0.8196]]],\n",
      "\n",
      "\n",
      "         [[[0.8252]]],\n",
      "\n",
      "\n",
      "         [[[0.8099]]],\n",
      "\n",
      "\n",
      "         [[[0.8249]]],\n",
      "\n",
      "\n",
      "         [[[0.8487]]],\n",
      "\n",
      "\n",
      "         [[[0.8107]]],\n",
      "\n",
      "\n",
      "         [[[0.7917]]],\n",
      "\n",
      "\n",
      "         [[[0.8777]]],\n",
      "\n",
      "\n",
      "         [[[0.8501]]],\n",
      "\n",
      "\n",
      "         [[[0.8421]]],\n",
      "\n",
      "\n",
      "         [[[0.7999]]],\n",
      "\n",
      "\n",
      "         [[[0.8102]]],\n",
      "\n",
      "\n",
      "         [[[0.8002]]],\n",
      "\n",
      "\n",
      "         [[[0.8344]]],\n",
      "\n",
      "\n",
      "         [[[0.8015]]],\n",
      "\n",
      "\n",
      "         [[[0.8192]]],\n",
      "\n",
      "\n",
      "         [[[0.8219]]],\n",
      "\n",
      "\n",
      "         [[[0.8504]]],\n",
      "\n",
      "\n",
      "         [[[0.7854]]],\n",
      "\n",
      "\n",
      "         [[[0.8088]]],\n",
      "\n",
      "\n",
      "         [[[0.8378]]],\n",
      "\n",
      "\n",
      "         [[[0.8230]]],\n",
      "\n",
      "\n",
      "         [[[0.8039]]],\n",
      "\n",
      "\n",
      "         [[[0.8512]]],\n",
      "\n",
      "\n",
      "         [[[0.8061]]],\n",
      "\n",
      "\n",
      "         [[[0.8523]]],\n",
      "\n",
      "\n",
      "         [[[0.8416]]],\n",
      "\n",
      "\n",
      "         [[[0.8128]]],\n",
      "\n",
      "\n",
      "         [[[0.7766]]],\n",
      "\n",
      "\n",
      "         [[[0.8130]]],\n",
      "\n",
      "\n",
      "         [[[0.8231]]],\n",
      "\n",
      "\n",
      "         [[[0.8091]]],\n",
      "\n",
      "\n",
      "         [[[0.8540]]],\n",
      "\n",
      "\n",
      "         [[[0.8478]]],\n",
      "\n",
      "\n",
      "         [[[0.8379]]],\n",
      "\n",
      "\n",
      "         [[[0.8530]]],\n",
      "\n",
      "\n",
      "         [[[0.8166]]],\n",
      "\n",
      "\n",
      "         [[[0.8688]]],\n",
      "\n",
      "\n",
      "         [[[0.8343]]],\n",
      "\n",
      "\n",
      "         [[[0.8281]]],\n",
      "\n",
      "\n",
      "         [[[0.8100]]],\n",
      "\n",
      "\n",
      "         [[[0.7981]]],\n",
      "\n",
      "\n",
      "         [[[0.8518]]],\n",
      "\n",
      "\n",
      "         [[[0.7911]]],\n",
      "\n",
      "\n",
      "         [[[0.8559]]],\n",
      "\n",
      "\n",
      "         [[[0.8056]]],\n",
      "\n",
      "\n",
      "         [[[0.8104]]],\n",
      "\n",
      "\n",
      "         [[[0.8600]]],\n",
      "\n",
      "\n",
      "         [[[0.7877]]],\n",
      "\n",
      "\n",
      "         [[[0.8232]]],\n",
      "\n",
      "\n",
      "         [[[0.8656]]],\n",
      "\n",
      "\n",
      "         [[[0.7862]]],\n",
      "\n",
      "\n",
      "         [[[0.8356]]],\n",
      "\n",
      "\n",
      "         [[[0.8191]]],\n",
      "\n",
      "\n",
      "         [[[0.8429]]],\n",
      "\n",
      "\n",
      "         [[[0.8267]]],\n",
      "\n",
      "\n",
      "         [[[0.8707]]],\n",
      "\n",
      "\n",
      "         [[[0.8462]]],\n",
      "\n",
      "\n",
      "         [[[0.8337]]],\n",
      "\n",
      "\n",
      "         [[[0.8292]]],\n",
      "\n",
      "\n",
      "         [[[0.8440]]],\n",
      "\n",
      "\n",
      "         [[[0.8445]]],\n",
      "\n",
      "\n",
      "         [[[0.8933]]],\n",
      "\n",
      "\n",
      "         [[[0.8051]]],\n",
      "\n",
      "\n",
      "         [[[0.8568]]],\n",
      "\n",
      "\n",
      "         [[[0.8105]]],\n",
      "\n",
      "\n",
      "         [[[0.8055]]],\n",
      "\n",
      "\n",
      "         [[[0.8157]]],\n",
      "\n",
      "\n",
      "         [[[0.7992]]],\n",
      "\n",
      "\n",
      "         [[[0.8270]]],\n",
      "\n",
      "\n",
      "         [[[0.7993]]],\n",
      "\n",
      "\n",
      "         [[[0.8857]]],\n",
      "\n",
      "\n",
      "         [[[0.7949]]],\n",
      "\n",
      "\n",
      "         [[[0.7938]]],\n",
      "\n",
      "\n",
      "         [[[0.8219]]],\n",
      "\n",
      "\n",
      "         [[[0.8337]]],\n",
      "\n",
      "\n",
      "         [[[0.8207]]],\n",
      "\n",
      "\n",
      "         [[[0.8461]]],\n",
      "\n",
      "\n",
      "         [[[0.8188]]],\n",
      "\n",
      "\n",
      "         [[[0.8555]]],\n",
      "\n",
      "\n",
      "         [[[0.8293]]],\n",
      "\n",
      "\n",
      "         [[[0.8219]]],\n",
      "\n",
      "\n",
      "         [[[0.8019]]],\n",
      "\n",
      "\n",
      "         [[[0.8192]]],\n",
      "\n",
      "\n",
      "         [[[0.8282]]],\n",
      "\n",
      "\n",
      "         [[[0.7864]]],\n",
      "\n",
      "\n",
      "         [[[0.7869]]],\n",
      "\n",
      "\n",
      "         [[[0.8058]]],\n",
      "\n",
      "\n",
      "         [[[0.7873]]],\n",
      "\n",
      "\n",
      "         [[[0.8179]]],\n",
      "\n",
      "\n",
      "         [[[0.8096]]],\n",
      "\n",
      "\n",
      "         [[[0.8287]]],\n",
      "\n",
      "\n",
      "         [[[0.8122]]],\n",
      "\n",
      "\n",
      "         [[[0.8599]]],\n",
      "\n",
      "\n",
      "         [[[0.8277]]],\n",
      "\n",
      "\n",
      "         [[[0.8114]]],\n",
      "\n",
      "\n",
      "         [[[0.8459]]],\n",
      "\n",
      "\n",
      "         [[[0.8466]]],\n",
      "\n",
      "\n",
      "         [[[0.8190]]],\n",
      "\n",
      "\n",
      "         [[[0.8085]]],\n",
      "\n",
      "\n",
      "         [[[0.8312]]],\n",
      "\n",
      "\n",
      "         [[[0.8714]]],\n",
      "\n",
      "\n",
      "         [[[0.8081]]],\n",
      "\n",
      "\n",
      "         [[[0.8231]]],\n",
      "\n",
      "\n",
      "         [[[0.8430]]],\n",
      "\n",
      "\n",
      "         [[[0.7883]]],\n",
      "\n",
      "\n",
      "         [[[0.8269]]],\n",
      "\n",
      "\n",
      "         [[[0.8374]]],\n",
      "\n",
      "\n",
      "         [[[0.7954]]],\n",
      "\n",
      "\n",
      "         [[[0.7946]]],\n",
      "\n",
      "\n",
      "         [[[0.8297]]],\n",
      "\n",
      "\n",
      "         [[[0.9034]]],\n",
      "\n",
      "\n",
      "         [[[0.8048]]],\n",
      "\n",
      "\n",
      "         [[[0.8304]]],\n",
      "\n",
      "\n",
      "         [[[0.8546]]],\n",
      "\n",
      "\n",
      "         [[[0.7989]]],\n",
      "\n",
      "\n",
      "         [[[0.8080]]],\n",
      "\n",
      "\n",
      "         [[[0.8277]]],\n",
      "\n",
      "\n",
      "         [[[0.8120]]],\n",
      "\n",
      "\n",
      "         [[[0.8358]]],\n",
      "\n",
      "\n",
      "         [[[0.8266]]],\n",
      "\n",
      "\n",
      "         [[[0.8478]]],\n",
      "\n",
      "\n",
      "         [[[0.8121]]],\n",
      "\n",
      "\n",
      "         [[[0.8117]]],\n",
      "\n",
      "\n",
      "         [[[0.8108]]],\n",
      "\n",
      "\n",
      "         [[[0.8528]]],\n",
      "\n",
      "\n",
      "         [[[0.8168]]],\n",
      "\n",
      "\n",
      "         [[[0.7885]]],\n",
      "\n",
      "\n",
      "         [[[0.8051]]],\n",
      "\n",
      "\n",
      "         [[[0.7953]]],\n",
      "\n",
      "\n",
      "         [[[0.8137]]],\n",
      "\n",
      "\n",
      "         [[[0.8122]]],\n",
      "\n",
      "\n",
      "         [[[0.8361]]],\n",
      "\n",
      "\n",
      "         [[[0.7382]]],\n",
      "\n",
      "\n",
      "         [[[0.8114]]],\n",
      "\n",
      "\n",
      "         [[[0.8215]]],\n",
      "\n",
      "\n",
      "         [[[0.8331]]],\n",
      "\n",
      "\n",
      "         [[[0.8127]]],\n",
      "\n",
      "\n",
      "         [[[0.8039]]],\n",
      "\n",
      "\n",
      "         [[[0.8195]]],\n",
      "\n",
      "\n",
      "         [[[0.8092]]],\n",
      "\n",
      "\n",
      "         [[[0.7950]]],\n",
      "\n",
      "\n",
      "         [[[0.8012]]],\n",
      "\n",
      "\n",
      "         [[[0.7866]]],\n",
      "\n",
      "\n",
      "         [[[0.8497]]],\n",
      "\n",
      "\n",
      "         [[[0.8357]]],\n",
      "\n",
      "\n",
      "         [[[0.8162]]],\n",
      "\n",
      "\n",
      "         [[[0.8391]]],\n",
      "\n",
      "\n",
      "         [[[0.8274]]],\n",
      "\n",
      "\n",
      "         [[[0.8042]]],\n",
      "\n",
      "\n",
      "         [[[0.7885]]],\n",
      "\n",
      "\n",
      "         [[[0.8126]]],\n",
      "\n",
      "\n",
      "         [[[0.8071]]],\n",
      "\n",
      "\n",
      "         [[[0.8423]]],\n",
      "\n",
      "\n",
      "         [[[0.8293]]],\n",
      "\n",
      "\n",
      "         [[[0.8264]]],\n",
      "\n",
      "\n",
      "         [[[0.7895]]],\n",
      "\n",
      "\n",
      "         [[[0.7935]]],\n",
      "\n",
      "\n",
      "         [[[0.8212]]],\n",
      "\n",
      "\n",
      "         [[[0.8352]]],\n",
      "\n",
      "\n",
      "         [[[0.8510]]],\n",
      "\n",
      "\n",
      "         [[[0.8049]]],\n",
      "\n",
      "\n",
      "         [[[0.8110]]],\n",
      "\n",
      "\n",
      "         [[[0.8056]]],\n",
      "\n",
      "\n",
      "         [[[0.8959]]],\n",
      "\n",
      "\n",
      "         [[[0.8406]]],\n",
      "\n",
      "\n",
      "         [[[0.8118]]],\n",
      "\n",
      "\n",
      "         [[[0.8285]]],\n",
      "\n",
      "\n",
      "         [[[0.8711]]],\n",
      "\n",
      "\n",
      "         [[[0.7983]]],\n",
      "\n",
      "\n",
      "         [[[0.8365]]],\n",
      "\n",
      "\n",
      "         [[[0.8043]]],\n",
      "\n",
      "\n",
      "         [[[0.7924]]],\n",
      "\n",
      "\n",
      "         [[[0.8159]]],\n",
      "\n",
      "\n",
      "         [[[0.7842]]],\n",
      "\n",
      "\n",
      "         [[[0.7768]]],\n",
      "\n",
      "\n",
      "         [[[0.8738]]],\n",
      "\n",
      "\n",
      "         [[[0.8117]]],\n",
      "\n",
      "\n",
      "         [[[0.8008]]],\n",
      "\n",
      "\n",
      "         [[[0.7998]]],\n",
      "\n",
      "\n",
      "         [[[0.8768]]],\n",
      "\n",
      "\n",
      "         [[[0.8124]]],\n",
      "\n",
      "\n",
      "         [[[0.8262]]],\n",
      "\n",
      "\n",
      "         [[[0.7857]]],\n",
      "\n",
      "\n",
      "         [[[0.8543]]],\n",
      "\n",
      "\n",
      "         [[[0.8101]]],\n",
      "\n",
      "\n",
      "         [[[0.7919]]],\n",
      "\n",
      "\n",
      "         [[[0.8628]]],\n",
      "\n",
      "\n",
      "         [[[0.7978]]],\n",
      "\n",
      "\n",
      "         [[[0.8040]]],\n",
      "\n",
      "\n",
      "         [[[0.8249]]],\n",
      "\n",
      "\n",
      "         [[[0.7974]]],\n",
      "\n",
      "\n",
      "         [[[0.7985]]],\n",
      "\n",
      "\n",
      "         [[[0.8575]]],\n",
      "\n",
      "\n",
      "         [[[0.8504]]],\n",
      "\n",
      "\n",
      "         [[[0.7993]]],\n",
      "\n",
      "\n",
      "         [[[0.8062]]],\n",
      "\n",
      "\n",
      "         [[[0.8300]]],\n",
      "\n",
      "\n",
      "         [[[0.8085]]],\n",
      "\n",
      "\n",
      "         [[[0.8100]]],\n",
      "\n",
      "\n",
      "         [[[0.8263]]],\n",
      "\n",
      "\n",
      "         [[[0.8159]]],\n",
      "\n",
      "\n",
      "         [[[0.7892]]],\n",
      "\n",
      "\n",
      "         [[[0.8539]]],\n",
      "\n",
      "\n",
      "         [[[0.7925]]],\n",
      "\n",
      "\n",
      "         [[[0.8244]]],\n",
      "\n",
      "\n",
      "         [[[0.8426]]],\n",
      "\n",
      "\n",
      "         [[[0.8261]]],\n",
      "\n",
      "\n",
      "         [[[0.8275]]],\n",
      "\n",
      "\n",
      "         [[[0.8762]]],\n",
      "\n",
      "\n",
      "         [[[0.8561]]],\n",
      "\n",
      "\n",
      "         [[[0.7518]]],\n",
      "\n",
      "\n",
      "         [[[0.8356]]],\n",
      "\n",
      "\n",
      "         [[[0.8149]]],\n",
      "\n",
      "\n",
      "         [[[0.8171]]],\n",
      "\n",
      "\n",
      "         [[[0.8507]]],\n",
      "\n",
      "\n",
      "         [[[0.8073]]],\n",
      "\n",
      "\n",
      "         [[[0.8245]]],\n",
      "\n",
      "\n",
      "         [[[0.7894]]],\n",
      "\n",
      "\n",
      "         [[[0.8190]]],\n",
      "\n",
      "\n",
      "         [[[0.8327]]],\n",
      "\n",
      "\n",
      "         [[[0.8067]]],\n",
      "\n",
      "\n",
      "         [[[0.7925]]],\n",
      "\n",
      "\n",
      "         [[[0.7904]]],\n",
      "\n",
      "\n",
      "         [[[0.8091]]],\n",
      "\n",
      "\n",
      "         [[[0.8161]]],\n",
      "\n",
      "\n",
      "         [[[0.8650]]],\n",
      "\n",
      "\n",
      "         [[[0.8167]]],\n",
      "\n",
      "\n",
      "         [[[0.8316]]],\n",
      "\n",
      "\n",
      "         [[[0.8763]]],\n",
      "\n",
      "\n",
      "         [[[0.8030]]],\n",
      "\n",
      "\n",
      "         [[[0.8145]]],\n",
      "\n",
      "\n",
      "         [[[0.8843]]],\n",
      "\n",
      "\n",
      "         [[[0.8144]]],\n",
      "\n",
      "\n",
      "         [[[0.8253]]],\n",
      "\n",
      "\n",
      "         [[[0.8193]]],\n",
      "\n",
      "\n",
      "         [[[0.8450]]],\n",
      "\n",
      "\n",
      "         [[[0.8909]]],\n",
      "\n",
      "\n",
      "         [[[0.7501]]],\n",
      "\n",
      "\n",
      "         [[[0.8007]]],\n",
      "\n",
      "\n",
      "         [[[0.8341]]],\n",
      "\n",
      "\n",
      "         [[[0.8033]]],\n",
      "\n",
      "\n",
      "         [[[0.7965]]],\n",
      "\n",
      "\n",
      "         [[[0.8067]]],\n",
      "\n",
      "\n",
      "         [[[0.7997]]],\n",
      "\n",
      "\n",
      "         [[[0.8051]]]]], device='cuda:0', grad_fn=<GatherBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.bn1.running_mean', 'module.bn1.running_var', 'module.bn1.num_batches_tracked', 'module.layer1.0.conv1.weight', 'module.layer1.0.bn1.weight', 'module.layer1.0.bn1.bias', 'module.layer1.0.bn1.running_mean', 'module.layer1.0.bn1.running_var', 'module.layer1.0.bn1.num_batches_tracked', 'module.layer1.0.conv2.weight', 'module.layer1.0.bn2.weight', 'module.layer1.0.bn2.bias', 'module.layer1.0.bn2.running_mean', 'module.layer1.0.bn2.running_var', 'module.layer1.0.bn2.num_batches_tracked', 'module.layer1.1.conv1.weight', 'module.layer1.1.bn1.weight', 'module.layer1.1.bn1.bias', 'module.layer1.1.bn1.running_mean', 'module.layer1.1.bn1.running_var', 'module.layer1.1.bn1.num_batches_tracked', 'module.layer1.1.conv2.weight', 'module.layer1.1.bn2.weight', 'module.layer1.1.bn2.bias', 'module.layer1.1.bn2.running_mean', 'module.layer1.1.bn2.running_var', 'module.layer1.1.bn2.num_batches_tracked', 'module.layer2.0.conv1.weight', 'module.layer2.0.bn1.weight', 'module.layer2.0.bn1.bias', 'module.layer2.0.bn1.running_mean', 'module.layer2.0.bn1.running_var', 'module.layer2.0.bn1.num_batches_tracked', 'module.layer2.0.conv2.weight', 'module.layer2.0.bn2.weight', 'module.layer2.0.bn2.bias', 'module.layer2.0.bn2.running_mean', 'module.layer2.0.bn2.running_var', 'module.layer2.0.bn2.num_batches_tracked', 'module.layer2.1.conv1.weight', 'module.layer2.1.bn1.weight', 'module.layer2.1.bn1.bias', 'module.layer2.1.bn1.running_mean', 'module.layer2.1.bn1.running_var', 'module.layer2.1.bn1.num_batches_tracked', 'module.layer2.1.conv2.weight', 'module.layer2.1.bn2.weight', 'module.layer2.1.bn2.bias', 'module.layer2.1.bn2.running_mean', 'module.layer2.1.bn2.running_var', 'module.layer2.1.bn2.num_batches_tracked', 'module.layer3.0.conv1.weight', 'module.layer3.0.bn1.weight', 'module.layer3.0.bn1.bias', 'module.layer3.0.bn1.running_mean', 'module.layer3.0.bn1.running_var', 'module.layer3.0.bn1.num_batches_tracked', 'module.layer3.0.conv2.weight', 'module.layer3.0.bn2.weight', 'module.layer3.0.bn2.bias', 'module.layer3.0.bn2.running_mean', 'module.layer3.0.bn2.running_var', 'module.layer3.0.bn2.num_batches_tracked', 'module.layer3.1.conv1.weight', 'module.layer3.1.bn1.weight', 'module.layer3.1.bn1.bias', 'module.layer3.1.bn1.running_mean', 'module.layer3.1.bn1.running_var', 'module.layer3.1.bn1.num_batches_tracked', 'module.layer3.1.conv2.weight', 'module.layer3.1.bn2.weight', 'module.layer3.1.bn2.bias', 'module.layer3.1.bn2.running_mean', 'module.layer3.1.bn2.running_var', 'module.layer3.1.bn2.num_batches_tracked', 'module.layer4.0.conv1.weight', 'module.layer4.0.bn1.weight', 'module.layer4.0.bn1.bias', 'module.layer4.0.bn1.running_mean', 'module.layer4.0.bn1.running_var', 'module.layer4.0.bn1.num_batches_tracked', 'module.layer4.0.conv2.weight', 'module.layer4.0.bn2.weight', 'module.layer4.0.bn2.bias', 'module.layer4.0.bn2.running_mean', 'module.layer4.0.bn2.running_var', 'module.layer4.0.bn2.num_batches_tracked', 'module.layer4.1.conv1.weight', 'module.layer4.1.bn1.weight', 'module.layer4.1.bn1.bias', 'module.layer4.1.bn1.running_mean', 'module.layer4.1.bn1.running_var', 'module.layer4.1.bn1.num_batches_tracked', 'module.layer4.1.conv2.weight', 'module.layer4.1.bn2.weight', 'module.layer4.1.bn2.bias', 'module.layer4.1.bn2.running_mean', 'module.layer4.1.bn2.running_var', 'module.layer4.1.bn2.num_batches_tracked'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4828/2631500724.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/christoph/Dokumente/christoph-MA/MedicalNet/pretrain/resnet_18.pth\"\n",
    "\n",
    "data_path = \"/storage/Datensätze\"\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(model_path)\n",
    "\n",
    "# Extract the nested state dictionary if it exists\n",
    "if 'state_dict' in state_dict:\n",
    "    state_dict = state_dict['state_dict']\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 4, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a dictionary to store outputs\n",
    "outputs = {}\n",
    "\n",
    "# Hook function to store layer outputs\n",
    "def hook_fn(module, input, output):\n",
    "    outputs[\"layer4\"] = output\n",
    "\n",
    "# Register hook on layer4\n",
    "model.module.layer4.register_forward_hook(hook_fn)\n",
    "\n",
    "# Create a dummy input tensor matching the model's expected input shape\n",
    "dummy_input = torch.randn(1, 1, 32, 128, 128).to(\"cuda\")  # (batch, channels, depth, height, width)\n",
    "\n",
    "# Forward pass\n",
    "model(dummy_input)\n",
    "\n",
    "# Print the captured features from layer4\n",
    "print(outputs[\"layer4\"].shape)  #\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "zip_file = '/home/christoph/Dokumente/christoph-MA/MedicalNet/MedicalNet_pytorch_files2.zip'\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
